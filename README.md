# ğŸ’¡ IIITD ChatBot: Stage 2 - Your Advanced RAG Assistant ğŸš€

Welcome to **Stage 2** of the AskAlma project! AskAlma is an intelligent, Retrieval-Augmented Generation (RAG) chatbot meticulously crafted to navigate the vast information landscape of the IIIT-Delhi website. This enhanced version builds upon our foundational work (see `AskAlma_Report_Stage1.pdf`) to deliver even more accurate, context-aware, and conversationally adept responses.

ğŸ”— **Live Demo:** [Link to your deployed demo - e.g., Vercel, Netlify] *(Coming Soon!)*

## ğŸŒŸ Project Vision

The IIIT-Delhi website is a rich repository of information. AskAlma's mission is to make this information instantly accessible and understandable for everyone â€“ from prospective students exploring programs to faculty seeking specific guidelines. We aim to transform how users interact with institutional knowledge.

## âœ¨ Stage 2 Highlights & Features

This iteration introduces a sophisticated pipeline designed for precision and conversational intelligence:

*   ğŸ§© **Hybrid Data Chunking:**
    *   Critical data like detailed course information (from JSONs ğŸ“„ and LLM-generated explanations âœï¸) is preserved as whole, coherent units.
    *   General content (PDFs  FOLDERSYMBOL, HTML ğŸŒ, etc.) is intelligently segmented for optimal retrieval.
*   ğŸ§  **LLM-Powered Query Understanding:**
    *   User queries within a conversation are dynamically rewritten by an LLM to be standalone and contextually grounded. This tackles ambiguous follow-ups (e.g., "tell me more about *it*") head-on!
*   ğŸ” **Two-Stage Hybrid Retrieval Powerhouse:**
    1.  **BM25 Keyword Search:** Lightning-fast âš¡ initial retrieval of candidate documents using strong keyword matching.
    2.  **Cross-Encoder Semantic Re-ranking:** Candidates are then semantically re-ranked by a powerful Cross-Encoder model, ensuring deep relevance to the user's nuanced intent.
*   ğŸ¤– **Modular LLM Interaction:**
    *   Final answers are generated by a separate LLM (we recommend using [LM Studio](https://lmstudio.ai/) for local hosting of models like `deepseek-r1-distill-qwen-7B`). This allows for easy model swapping and experimentation.

## ğŸ“ˆ From Stage 1 to Stage 2: Our Evolution

Our initial vision (detailed in `AskAlma_Report_Stage1.pdf`) included components like Graph RAG. Here's how we've adapted and advanced in Stage 2:

*   **Enhanced Course Data Handling:** While Graph RAG remains a future goal, we've implemented a robust system for course information. Structured course JSONs and supplementary LLM-generated textual explanations are treated as critical, whole documents. This makes them highly discoverable through both keywords and semantic meaning.
*   **Intelligent Contextualization:** The LLM-powered query condensing step effectively provides the "relevance bias" initially envisioned for a separate classifier, guiding retrieval with rich conversational context.
*   **Focus on Core RAG Excellence:** This stage delivers a highly refined "Standard RAG Retriever" (as per our report's terminology) with cutting-edge conversational and hybrid search capabilities.

## ğŸ“‚ Repository Structure

*   `Askalma/`: ğŸ“š Your complete data corpus.
    *   `attachments/`: PDFs, DOCX, etc.
    *   `course_json/`:  ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ JSON course files (preserved whole).
    *   `course_explain/`: âœï¸ LLM-generated course explanations (preserved whole).
    *   `html/`, `tables/`, `text_pdfs/`: Other scraped data.
    *   `factual_data_spanbert.json`: Extracted facts (chunked generically).
*   `Frontend-askalma/`: ğŸ–¥ï¸ The Next.js frontend application.
*   `Scrapping-pipeline/`: ğŸ•¸ï¸ Jupyter notebooks & scripts for data scraping.
*   `rag_pipeline.py`: ğŸ The heart of AskAlma â€“ all Python RAG logic.
*   `main.py`: âš™ï¸ FastAPI backend server.
*   `README.md`: ğŸ“– You are here!
*   `INSTALLATION.md`: ğŸ› ï¸ Setup instructions.
*   `requirements.txt`: ğŸ“‹ Python dependencies.
*   `AskAlma_Report_Stage1.pdf`: ğŸ“„ Our initial project report.

## ğŸ› ï¸ Tech Stack

*   **Backend:** Python, FastAPI
*   **Frontend:** Next.js, React, TypeScript, Tailwind CSS
*   **RAG Pipeline:** LangChain (LCEL)
*   **Retrieval:** BM25, Cross-Encoders (`sentence-transformers`)
*   **LLMs (Local via LM Studio):**
    *   Query Condensing & Answer Generation: `deepseek-r1-distill-qwen-7B` (or similar GGUF)
    *   Offline data processing (JSON structuring, initial explanations)
*   **Data Processing:** PyPDF2, python-docx, pandas, BeautifulSoup
*   **Windows Specific:** `pywin32` (for `.doc` conversion)

## ğŸš€ Getting Started

Ready to see AskAlma in action? Head over to our `INSTALLATION.md` for detailed setup steps!

## ğŸ’» Usage

1.  **Backend & RAG:** Follow `INSTALLATION.md`.
2.  **Start Backend:** In the project root:
    ```bash
    python -m uvicorn main:app --reload
    ```
3.  **Start Frontend:** In `Frontend-askalma/`:
    ```bash
    npm install && npm run dev
    ```
    Access at `http://localhost:3000`.
4.  **Direct RAG Pipeline Test:** In the project root:
    ```bash
    python rag_pipeline.py
    ```

## ğŸŒ± Future Work

Our journey with AskAlma continues! We're exploring:
*   Expanding the Knowledge Base ğŸ“š
*   Implementing full Graph RAG for courses ğŸ•¸ï¸
*   Adding a dedicated Small LLM Relevance Classifier ğŸ¯
*   Rigorous RAGAS Evaluation ğŸ“Š
*   Latency Reduction & Caching âš¡
*   User Feedback Integration ğŸ‘

(See `AskAlma_Report_Stage1.pdf` for more details on future scope)

## ğŸ¤ The Team & Contribution

AskAlma was proudly developed by:
*   Akshat Kothari
*   Vinayak Agrawal

We welcome insights and contributions from the community! (Please refer to standard GitHub practices for contributions).
