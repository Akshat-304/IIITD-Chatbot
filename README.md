# 💡 IIITD ChatBot: Stage 2 - Your Advanced RAG Assistant 🚀

Welcome to **Stage 2** of the AskAlma project! AskAlma is an intelligent, Retrieval-Augmented Generation (RAG) chatbot meticulously crafted to navigate the vast information landscape of the IIIT-Delhi website. This enhanced version builds upon our foundational work (see `AskAlma_Report_Stage1.pdf`) to deliver even more accurate, context-aware, and conversationally adept responses.

🔗 **Live Demo:** [Link to your deployed demo - e.g., Vercel, Netlify] *(Coming Soon!)*

## 🌟 Project Vision

The IIIT-Delhi website is a rich repository of information. AskAlma's mission is to make this information instantly accessible and understandable for everyone – from prospective students exploring programs to faculty seeking specific guidelines. We aim to transform how users interact with institutional knowledge.

## ✨ Stage 2 Highlights & Features

This iteration introduces a sophisticated pipeline designed for precision and conversational intelligence:

*   🧩 **Hybrid Data Chunking:**
    *   Critical data like detailed course information (from JSONs 📄 and LLM-generated explanations ✍️) is preserved as whole, coherent units.
    *   General content (PDFs  FOLDERSYMBOL, HTML 🌐, etc.) is intelligently segmented for optimal retrieval.
*   🧠 **LLM-Powered Query Understanding:**
    *   User queries within a conversation are dynamically rewritten by an LLM to be standalone and contextually grounded. This tackles ambiguous follow-ups (e.g., "tell me more about *it*") head-on!
*   🔍 **Two-Stage Hybrid Retrieval Powerhouse:**
    1.  **BM25 Keyword Search:** Lightning-fast ⚡ initial retrieval of candidate documents using strong keyword matching.
    2.  **Cross-Encoder Semantic Re-ranking:** Candidates are then semantically re-ranked by a powerful Cross-Encoder model, ensuring deep relevance to the user's nuanced intent.
*   🤖 **Modular LLM Interaction:**
    *   Final answers are generated by a separate LLM (we recommend using [LM Studio](https://lmstudio.ai/) for local hosting of models like `deepseek-r1-distill-qwen-7B`). This allows for easy model swapping and experimentation.

## 📈 From Stage 1 to Stage 2: Our Evolution

Our initial vision (detailed in `AskAlma_Report_Stage1.pdf`) included components like Graph RAG. Here's how we've adapted and advanced in Stage 2:

*   **Enhanced Course Data Handling:** While Graph RAG remains a future goal, we've implemented a robust system for course information. Structured course JSONs and supplementary LLM-generated textual explanations are treated as critical, whole documents. This makes them highly discoverable through both keywords and semantic meaning.
*   **Intelligent Contextualization:** The LLM-powered query condensing step effectively provides the "relevance bias" initially envisioned for a separate classifier, guiding retrieval with rich conversational context.
*   **Focus on Core RAG Excellence:** This stage delivers a highly refined "Standard RAG Retriever" (as per our report's terminology) with cutting-edge conversational and hybrid search capabilities.

## 📂 Repository Structure

*   `Askalma/`: 📚 Your complete data corpus.
    *   `attachments/`: PDFs, DOCX, etc.
    *   `course_json/`:  критический JSON course files (preserved whole).
    *   `course_explain/`: ✍️ LLM-generated course explanations (preserved whole).
    *   `html/`, `tables/`, `text_pdfs/`: Other scraped data.
    *   `factual_data_spanbert.json`: Extracted facts (chunked generically).
*   `Frontend-askalma/`: 🖥️ The Next.js frontend application.
*   `Scrapping-pipeline/`: 🕸️ Jupyter notebooks & scripts for data scraping.
*   `rag_pipeline.py`: 🐍 The heart of AskAlma – all Python RAG logic.
*   `main.py`: ⚙️ FastAPI backend server.
*   `README.md`: 📖 You are here!
*   `INSTALLATION.md`: 🛠️ Setup instructions.
*   `requirements.txt`: 📋 Python dependencies.
*   `AskAlma_Report_Stage1.pdf`: 📄 Our initial project report.

## 🛠️ Tech Stack

*   **Backend:** Python, FastAPI
*   **Frontend:** Next.js, React, TypeScript, Tailwind CSS
*   **RAG Pipeline:** LangChain (LCEL)
*   **Retrieval:** BM25, Cross-Encoders (`sentence-transformers`)
*   **LLMs (Local via LM Studio):**
    *   Query Condensing & Answer Generation: `deepseek-r1-distill-qwen-7B` (or similar GGUF)
    *   Offline data processing (JSON structuring, initial explanations)
*   **Data Processing:** PyPDF2, python-docx, pandas, BeautifulSoup
*   **Windows Specific:** `pywin32` (for `.doc` conversion)

## 🚀 Getting Started

Ready to see AskAlma in action? Head over to our `INSTALLATION.md` for detailed setup steps!

## 💻 Usage

1.  **Backend & RAG:** Follow `INSTALLATION.md`.
2.  **Start Backend:** In the project root:
    ```bash
    python -m uvicorn main:app --reload
    ```
3.  **Start Frontend:** In `Frontend-askalma/`:
    ```bash
    npm install && npm run dev
    ```
    Access at `http://localhost:3000`.
4.  **Direct RAG Pipeline Test:** In the project root:
    ```bash
    python rag_pipeline.py
    ```

## 🌱 Future Work

Our journey with AskAlma continues! We're exploring:
*   Expanding the Knowledge Base 📚
*   Implementing full Graph RAG for courses 🕸️
*   Adding a dedicated Small LLM Relevance Classifier 🎯
*   Rigorous RAGAS Evaluation 📊
*   Latency Reduction & Caching ⚡
*   User Feedback Integration 👍

(See `AskAlma_Report_Stage1.pdf` for more details on future scope)

## 🤝 The Team & Contribution

AskAlma was proudly developed by:
*   Akshat Kothari
*   Vinayak Agrawal

We welcome insights and contributions from the community! (Please refer to standard GitHub practices for contributions).
